{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Notebook\n",
    "=================\n",
    "\n",
    "This notebook consist of 4 section below:\n",
    "\n",
    "1. Load libraries\n",
    "1. Define the code for training\n",
    "1. Load data\n",
    "1. Run training\n",
    "\n",
    "When you want to attempt more training, please modify section 3 and 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load libraries\n",
    "--------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "from glob import glob\n",
    "from random import shuffle\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from tensorflow.keras.applications import DenseNet121\n",
    "from tensorflow.keras.applications.densenet import preprocess_input\n",
    "from tensorflow.keras.callbacks import (CSVLogger, EarlyStopping, ModelCheckpoint, TensorBoard)\n",
    "from tensorflow.keras.layers import (AvgPool2D, Conv2D, Dense, GlobalAvgPool2D, Input)\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the code for training\n",
    "----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filelist, rgb=False):\n",
    "    images = (Image.open(f) for f in filelist)\n",
    "    if rgb:\n",
    "        images = (img.convert('RGB') for img in images)\n",
    "    X = np.array([np.array(img) for img in images])\n",
    "    if rgb:\n",
    "        X = preprocess_input(X)\n",
    "    else:\n",
    "        X = np.expand_dims(X, axis=-1)\n",
    "        X = X.astype(np.float32)\n",
    "    return X\n",
    "\n",
    "\n",
    "def create_model(base_filters, conv_depth=4, dense_units=None, regularizer=None):\n",
    "    conv_layers = [base_filters * (2**i) for i in range(conv_depth)]\n",
    "    x = inputs = Input((None, None, 1), name='input')\n",
    "    for i, filters in enumerate(conv_layers):\n",
    "        x = Conv2D(filters,\n",
    "                   3,\n",
    "                   padding='same',\n",
    "                   activation='selu',\n",
    "                   use_bias=False,\n",
    "                   kernel_regularizer=None if regularizer is None else l2(regularizer),\n",
    "                   name='conv{}'.format(i + 1))(x)\n",
    "        if i + 1 < len(conv_layers):\n",
    "            x = AvgPool2D(name='pool{}'.format(i + 1))(x)\n",
    "        else:\n",
    "            x = GlobalAvgPool2D(name='global-pool')(x)\n",
    "    if dense_units is not None:\n",
    "        x = Dense(dense_units,\n",
    "                  activation='selu',\n",
    "                  kernel_regularizer=None if regularizer is None else l2(regularizer),\n",
    "                  name='dense')(x)\n",
    "    outputs = Dense(1, activation='sigmoid', name='output')(x)\n",
    "    model = Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_model(input_size, transfer_learning=False):\n",
    "    if transfer_learning:\n",
    "        base_model = DenseNet121(include_top=False,\n",
    "                                 input_shape=(input_size, input_size, 3),\n",
    "                                 pooling='avg')\n",
    "        base_model.trainable = False\n",
    "        x = base_model.output\n",
    "        x = Dense(100, activation='selu', name='dense1')(x)\n",
    "        outputs = Dense(1, activation='sigmoid', name='output')(x)\n",
    "        model = Model(inputs=base_model.input, outputs=outputs)\n",
    "    else:\n",
    "        if input_size <= 32:\n",
    "            layers = 3\n",
    "        elif input_size <= 64:\n",
    "            layers = 4\n",
    "        else:\n",
    "            layers = 5\n",
    "        model = create_model(32, conv_depth=layers, regularizer=0.001)\n",
    "    return model\n",
    "\n",
    "\n",
    "def mkdir(path):\n",
    "    if not os.path.isdir(path):\n",
    "        os.mkdir(path)\n",
    "        os.chmod(path, 755)\n",
    "        print('{} directory was created.'.format(path))\n",
    "\n",
    "\n",
    "def train(model, model_name, X, y, validation_data, batch_size, lr, enable_tensorboard=False):\n",
    "    _, input_size, _, channels = X.shape\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=tf.keras.optimizers.Adam(lr),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    datagen = ImageDataGenerator(rotation_range=5,\n",
    "                                 width_shift_range=10.0,\n",
    "                                 height_shift_range=10.0,\n",
    "                                 fill_mode='nearest',\n",
    "                                 cval=1.0,\n",
    "                                 horizontal_flip=True,\n",
    "                                 vertical_flip=True)\n",
    "    datagen.fit(X)\n",
    "\n",
    "    mkdir('results')\n",
    "    mkdir('results/checkpoints')\n",
    "    mkdir('results/logs')\n",
    "\n",
    "    if enable_tensorboard:\n",
    "        tensorboard = TensorBoard(log_dir=\"results/logs/{}\".format(model_name))\n",
    "    model_checkpoint = ModelCheckpoint('results/checkpoints/{val_loss:.5f}.' +\n",
    "                                       '{}'.format(model_name) + '.{epoch:02d}epoch.h5',\n",
    "                                       save_best_only=True)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
    "    csv_logger = CSVLogger('results/{}.csv'.format(model_name))\n",
    "\n",
    "    print(model_name)\n",
    "    return model.fit(\n",
    "        datagen.flow(X, y, batch_size=batch_size),\n",
    "        # batch_size=batch_size,\n",
    "        steps_per_epoch=len(X) / batch_size,\n",
    "        epochs=1000,\n",
    "        validation_data=validation_data,\n",
    "        callbacks=[model_checkpoint, early_stopping, csv_logger] +\n",
    "        ([tensorboard] if enable_tensorboard else []),\n",
    "        verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data\n",
    "---------\n",
    "\n",
    "This section is for loading training data that are 64 x 64 size images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2939, 2939, 735, 735)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files_O = glob('datasets/images_64/LSR_O/*.png')\n",    # directory for LSR_presence files
    "t_O = [0] * len(files_O)\n",
    "\n",
    "files_X = glob('datasets/images_64/LSR_X/*.png')\n",    # directory for LSR_absence files
    "t_X = [1] * len(files_X)\n",
    "\n",
    "files = files_O + files_X\n",
    "t = t_O + t_X\n",
    "\n",
    "data = [(f, y) for f, y in zip(files, t)]\n",
    "shuffle(data)\n",
    "\n",
    "split_index = int(len(data) * 0.8)\n",
    "data_train, data_val = data[:split_index], data[split_index:]\n",
    "\n",
    "X_train = load_data(list(map(lambda d: d[0], data_train)))\n",
    "t_train = np.array(list(map(lambda d: d[1], data_train)))\n",
    "X_val = load_data(list(map(lambda d: d[0], data_val)))\n",
    "t_val = np.array(list(map(lambda d: d[1], data_val)))\n",
    "\n",
    "len(X_train), len(t_train), len(X_val), len(t_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run training\n",
    "------------\n",
    "\n",
    "The hyper-parameters I've already attempted are:\n",
    "\n",
    "1. The depth of convolutional layers\n",
    "1. The number of filters of convolutional layer.\n",
    "1. Whether there is a Dense layer, and its size.\n",
    "1. The batch size for fitting\n",
    "1. The learning late of optimizer.\n",
    "1. Whether there are regularizers for each layers.\n",
    "\n",
    "**Note:**\n",
    "The model file (5layers-32filters-8batch-0.0003lr-None.57epoch.h5) I sent you was created old `Run training` section. While trial and error, I've modified below section. Finally, I added a hyper-parameter whether there is a Dense layer, and its size, but it was almost NOT effective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_depths = [5, 4, 3]\n",
    "base_filters = [64, 32, 16, 8]\n",
    "denses = [256, 128, 64, None]\n",
    "batch_sizes = [128, 64, 32, 16, 8]\n",
    "lrs = [0.001, 0.0003, 0.0001]\n",
    "regularizers = [None, 0.0001]\n",
    "\n",
    "for conv_depth in conv_depths:\n",
    "    for base_filter in base_filters:\n",
    "        for dense in denses:\n",
    "            for batch_size in batch_sizes:\n",
    "                for lr in lrs:\n",
    "                    for regularizer in regularizers:\n",
    "                        model_name = '{}layers-{}filters-{}dense-{}batch-{}lr-{}regularization'.format(\n",
    "                            conv_depth, base_filter, dense, batch_size, lr, regularizer)\n",
    "                        model = create_model(base_filter,\n",
    "                                             conv_depth=conv_depth,\n",
    "                                             dense_units=dense,\n",
    "                                             regularizer=regularizer)\n",
    "                        train(model, model_name, X_train, t_train, (X_val, t_val), batch_size, lr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
